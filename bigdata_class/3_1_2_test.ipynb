{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import requests\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver\")\n",
    "\n",
    "url = \"https://search.naver.com/search.naver?sm=tab_hty.top&where=image&query=고양이\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "images = (driver.find_elements(By.XPATH, '//img[@class=\"_image _listImage\"]'))\n",
    "print (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 스크롤 다운 행동\n",
    "for _ in range(3):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "savePath = \"/Users/chan6502/Desktop/image2/\"\n",
    "count = 0\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    if count >= 200:\n",
    "        break\n",
    "    \n",
    "    image_url = image.get_attribute(\"src\")\n",
    "    if image_url:\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            image_content = response.content\n",
    "\n",
    "            image_pil = Image.open(BytesIO(image_content))\n",
    "\n",
    "            file_name = f\"food_{count}.jpg\"\n",
    "            with open(savePath + file_name, \"wb\") as f:\n",
    "                image_pil.save(f, \"JPEG\")\n",
    "                print(f\"Saved {file_name}\")\n",
    "                count = count + 1\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver\")\n",
    "\n",
    "url = \"https://www.google.co.jp/search?q=%EA%B3%A0%EC%96%91%EC%9D%B4&hl=en&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiU9uHaho7-AhXHCN4KHddTAZ8Q_AUoAXoECAEQAw&biw=1194&bih=698&dpr=2\"\n",
    "driver.get(url)\n",
    "from selenium.webdriver.common.by import By\n",
    "images = (driver.find_elements(By.XPATH, '//img[@class=\"rg_i Q4LuWd\"]'))\n",
    "## 스크롤 다운 행동\n",
    "for _ in range(3):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "savePath = \"/Users/chan6502/Desktop/image2/\"\n",
    "count = 0\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    if count >= 200:\n",
    "        break\n",
    "    \n",
    "    image_url = image.get_attribute(\"src\")\n",
    "    if image_url:\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            image_content = response.content\n",
    "\n",
    "            image_pil = Image.open(BytesIO(image_content))\n",
    "\n",
    "            file_name = f\"food_{count}.jpg\"\n",
    "            with open(savePath + file_name, \"wb\") as f:\n",
    "                image_pil.save(f, \"JPEG\")\n",
    "                print(f\"Saved {file_name}\")\n",
    "                count = count + 1\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate(images):\n",
    "    if count >= 70:\n",
    "        break\n",
    "    \n",
    "    image_url = image.get_attribute(\"src\")\n",
    "    if image_url:\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            image_content = response.content\n",
    "\n",
    "            image_pil = Image.open(BytesIO(image_content))\n",
    "\n",
    "            file_name = f\"food_{count}.jpg\"\n",
    "            with open(savePath + file_name, \"wb\") as f:\n",
    "                image_pil.save(f, \"JPEG\")\n",
    "                print(f\"Saved {file_name}\")\n",
    "                count = count + 1\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "client_id = \"VVrSb7DXLYUr2IRlQNSU\"\n",
    "client_secret = \"pJSQFrP13Z\"\n",
    "encText = urllib.parse.quote(\"미국\")\n",
    "\n",
    "# 모든 페이지에 대해서 데이터 수집 및 엑셀 파일에 기록\n",
    "for page in range(1, 11):\n",
    "    url = f\"https://openapi.naver.com/v1/search/book?query={encText}&display=100&start={100*(page-1)+1}\"\n",
    "    \n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    context = ssl._create_unverified_context()\n",
    "    response = urllib.request.urlopen(request, context=context)\n",
    "    rescode = response.getcode()\n",
    "\n",
    "    if(rescode==200):\n",
    "        response_body = response.read()\n",
    "        result = response_body.decode('utf-8')\n",
    "        \n",
    "        book_result = json.loads(result)\n",
    "        \n",
    "        df = pd.DataFrame(book_result['items'])\n",
    "\n",
    "        read_data = pd.read_csv(\"naver_book.csv\", sep=\",\")\n",
    "        print (read_data)\n",
    "        df.to_csv(\"naver_book.csv\")\n",
    "\n",
    "        print(f\"페이지 {page} 책 데이터 수집 완료\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"페이지 {page} 책 데이터 수집 실패\")\n",
    "\n",
    "\n",
    "print(\"모든 페이지의 책 데이터 수집 완료\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
